/home/said.gurbuz/miniconda3/envs/fewshotbench/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'main': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
dataset:
  type: classification
  simple_cls:
    _target_: datasets.prot.swissprot.SPSimpleDataset
  set_cls:
    n_way: 5
    n_support: 5
    n_query: 15
    _target_: datasets.prot.swissprot.SPSetDataset
  name: swissprot
eval_split:
- train
- val
- test
backbone:
  _target_: backbones.fcnet.FCNet
  layer_dim:
  - 512
  - 512
train_classes: 7195
n_way: 5
n_shot: 5
n_query: 15
method:
  name: matchingnet
  train_batch: null
  val_batch: null
  fast_weight: false
  start_epoch: 0
  eval_type: set
  stop_epoch: 60
  type: meta
  cls:
    n_way: 5
    n_support: 5
    _target_: methods.matchingnet.MatchingNet
model: FCNet
mode: train
exp:
  name: method_matchingnet_dataset_swissprot_n_shot_5
  save_freq: 10
  resume: false
  seed: 42
  val_freq: 1
optimizer: Adam
lr: 0.001
optimizer_cls:
  _target_: torch.optim.Adam
  lr: 0.001
checkpoint:
  dir: checkpoints/method_matchingnet_dataset_swissprot_n_shot_5/swissprot/matchingnet_FCNet
  test_iter: best_model
  time: latest
wandb:
  project: leo
  entity: leo
  mode: disabled
iter_num: 600

  EXISTS: go-basic.obo
go-basic.obo: fmt(1.2) rel(2023-06-11) 46,420 Terms; optional_attrs(relationship)

**WARNING: NO VERSION LINE FOUND IN GAF FILE. USING:
!gaf-version: 2.2
[2023-12-09 01:30:18,912][root][ERROR] - Failed to validate header as GAF v2.2:
[]
HMS:0:00:07.275593 310,057 annotations READ: ./data/swissprot/filtered_goa_uniprot_all_noiea.gaf 
25933 IDs in loaded association branch, BP

**WARNING: NO VERSION LINE FOUND IN GAF FILE. USING:
!gaf-version: 2.2
[2023-12-09 01:31:47,922][root][ERROR] - Failed to validate header as GAF v2.2:
[]
HMS:0:00:07.732197 310,057 annotations READ: ./data/swissprot/filtered_goa_uniprot_all_noiea.gaf 
25933 IDs in loaded association branch, BP
Model Architecture:
MatchingNet(
  (feature): FCNet(
    (encoder): Sequential(
      (0): Sequential(
        (0): Linear(in_features=1280, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (loss_fn): NLLLoss()
  (FCE): FullyContextualEmbedding(
    (lstmcell): LSTMCell(1024, 512)
    (softmax): Softmax(dim=None)
  )
  (G_encoder): LSTM(512, 512, batch_first=True, bidirectional=True)
  (relu): ReLU()
  (softmax): Softmax(dim=None)
)
Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
/home/said.gurbuz/epfl-dl-in-biomed-project/methods/matchingnet.py:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  a = self.softmax(logit_a)
/home/said.gurbuz/epfl-dl-in-biomed-project/methods/matchingnet.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  softmax = self.softmax(scores)
Epoch 0 | Batch 0/100 | Loss 1.805483
Epoch 0 | Batch 10/100 | Loss 1.701448
Epoch 0 | Batch 20/100 | Loss 1.701818
Epoch 0 | Batch 30/100 | Loss 1.637296
Epoch 0 | Batch 40/100 | Loss 1.485379
Epoch 0 | Batch 50/100 | Loss 1.515008
Epoch 0 | Batch 60/100 | Loss 1.432063
Epoch 0 | Batch 70/100 | Loss 1.372359
Epoch 0 | Batch 80/100 | Loss 1.319224
Epoch 0 | Batch 90/100 | Loss 1.283026
100 Test Acc = 68.97% +- 2.03%
Epoch 0: 68.97
best model! save...
Epoch 1 | Batch 0/100 | Loss 0.431571
Epoch 1 | Batch 10/100 | Loss 0.878405
Epoch 1 | Batch 20/100 | Loss 0.853738
Epoch 1 | Batch 30/100 | Loss 0.829874
Epoch 1 | Batch 40/100 | Loss 0.738258
Epoch 1 | Batch 50/100 | Loss 0.728212
Epoch 1 | Batch 60/100 | Loss 0.718971
Epoch 1 | Batch 70/100 | Loss 0.728062
Epoch 1 | Batch 80/100 | Loss 0.725721
Epoch 1 | Batch 90/100 | Loss 0.714905
100 Test Acc = 70.25% +- 1.82%
Epoch 1: 70.25
best model! save...
Epoch 2 | Batch 0/100 | Loss 0.651356
Epoch 2 | Batch 10/100 | Loss 0.633750
Epoch 2 | Batch 20/100 | Loss 0.644128
Epoch 2 | Batch 30/100 | Loss 0.651890
Epoch 2 | Batch 40/100 | Loss 0.669851
Epoch 2 | Batch 50/100 | Loss 0.657501
Epoch 2 | Batch 60/100 | Loss 0.614701
Epoch 2 | Batch 70/100 | Loss 0.623559
Epoch 2 | Batch 80/100 | Loss 0.616188
Epoch 2 | Batch 90/100 | Loss 0.609991
100 Test Acc = 67.45% +- 1.56%
Epoch 2: 67.45
Epoch 3 | Batch 0/100 | Loss 1.074910
Epoch 3 | Batch 10/100 | Loss 0.558433
Epoch 3 | Batch 20/100 | Loss 0.538434
Epoch 3 | Batch 30/100 | Loss 0.543104
Epoch 3 | Batch 40/100 | Loss 0.541008
Epoch 3 | Batch 50/100 | Loss 0.541674
Epoch 3 | Batch 60/100 | Loss 0.527170
Epoch 3 | Batch 70/100 | Loss 0.540196
Epoch 3 | Batch 80/100 | Loss 0.533614
Epoch 3 | Batch 90/100 | Loss 0.523256
100 Test Acc = 69.75% +- 2.02%
Epoch 3: 69.75
Epoch 4 | Batch 0/100 | Loss 0.172859
Epoch 4 | Batch 10/100 | Loss 0.465071
Epoch 4 | Batch 20/100 | Loss 0.479983
Epoch 4 | Batch 30/100 | Loss 0.432918
Epoch 4 | Batch 40/100 | Loss 0.411178
Epoch 4 | Batch 50/100 | Loss 0.400771
Epoch 4 | Batch 60/100 | Loss 0.415495
Epoch 4 | Batch 70/100 | Loss 0.420728
Epoch 4 | Batch 80/100 | Loss 0.416320
Epoch 4 | Batch 90/100 | Loss 0.406362
100 Test Acc = 69.61% +- 1.83%
Epoch 4: 69.61
Epoch 5 | Batch 0/100 | Loss 0.379468
Epoch 5 | Batch 10/100 | Loss 0.356409
Epoch 5 | Batch 20/100 | Loss 0.368851
Epoch 5 | Batch 30/100 | Loss 0.386572
Epoch 5 | Batch 40/100 | Loss 0.432449
Epoch 5 | Batch 50/100 | Loss 0.408629
Epoch 5 | Batch 60/100 | Loss 0.418396
Epoch 5 | Batch 70/100 | Loss 0.418301
Epoch 5 | Batch 80/100 | Loss 0.406863
Epoch 5 | Batch 90/100 | Loss 0.403893
100 Test Acc = 67.20% +- 1.92%
Epoch 5: 67.20
Epoch 6 | Batch 0/100 | Loss 0.027139
Epoch 6 | Batch 10/100 | Loss 0.287564
Epoch 6 | Batch 20/100 | Loss 0.328429
Epoch 6 | Batch 30/100 | Loss 0.348395
Epoch 6 | Batch 40/100 | Loss 0.318411
Epoch 6 | Batch 50/100 | Loss 0.324084
Epoch 6 | Batch 60/100 | Loss 0.333949
Epoch 6 | Batch 70/100 | Loss 0.323117
Epoch 6 | Batch 80/100 | Loss 0.323543
Epoch 6 | Batch 90/100 | Loss 0.315034
100 Test Acc = 68.99% +- 1.86%
Epoch 6: 68.99
Epoch 7 | Batch 0/100 | Loss 0.149999
Epoch 7 | Batch 10/100 | Loss 0.299463
Epoch 7 | Batch 20/100 | Loss 0.298462
Epoch 7 | Batch 30/100 | Loss 0.310370
Epoch 7 | Batch 40/100 | Loss 0.309326
Epoch 7 | Batch 50/100 | Loss 0.320703
Epoch 7 | Batch 60/100 | Loss 0.325672
Epoch 7 | Batch 70/100 | Loss 0.338026
Epoch 7 | Batch 80/100 | Loss 0.348127
Epoch 7 | Batch 90/100 | Loss 0.342451
100 Test Acc = 69.67% +- 1.95%
Epoch 7: 69.67
Epoch 8 | Batch 0/100 | Loss 0.498433
Epoch 8 | Batch 10/100 | Loss 0.234832
Epoch 8 | Batch 20/100 | Loss 0.243547
Epoch 8 | Batch 30/100 | Loss 0.243325
Epoch 8 | Batch 40/100 | Loss 0.250141
Epoch 8 | Batch 50/100 | Loss 0.268322
Epoch 8 | Batch 60/100 | Loss 0.265768
Epoch 8 | Batch 70/100 | Loss 0.287801
Epoch 8 | Batch 80/100 | Loss 0.286281
Epoch 8 | Batch 90/100 | Loss 0.284667
100 Test Acc = 68.35% +- 2.09%
Epoch 8: 68.35
Epoch 9 | Batch 0/100 | Loss 0.091832
Epoch 9 | Batch 10/100 | Loss 0.439559
Epoch 9 | Batch 20/100 | Loss 0.334293
Epoch 9 | Batch 30/100 | Loss 0.266111
Epoch 9 | Batch 40/100 | Loss 0.270308
Epoch 9 | Batch 50/100 | Loss 0.269247
Epoch 9 | Batch 60/100 | Loss 0.259327
Epoch 9 | Batch 70/100 | Loss 0.263639
Epoch 9 | Batch 80/100 | Loss 0.267787
Epoch 9 | Batch 90/100 | Loss 0.252485
100 Test Acc = 71.01% +- 1.96%
Epoch 9: 71.01
best model! save...
Epoch 10 | Batch 0/100 | Loss 0.204069
Epoch 10 | Batch 10/100 | Loss 0.234779
Epoch 10 | Batch 20/100 | Loss 0.268243
Epoch 10 | Batch 30/100 | Loss 0.294966
Epoch 10 | Batch 40/100 | Loss 0.281004
Epoch 10 | Batch 50/100 | Loss 0.278149
Epoch 10 | Batch 60/100 | Loss 0.281108
Epoch 10 | Batch 70/100 | Loss 0.303235
Epoch 10 | Batch 80/100 | Loss 0.323066
Epoch 10 | Batch 90/100 | Loss 0.316290
100 Test Acc = 68.31% +- 2.02%
Epoch 10: 68.31
Epoch 11 | Batch 0/100 | Loss 0.211448
Epoch 11 | Batch 10/100 | Loss 0.130755
Epoch 11 | Batch 20/100 | Loss 0.179206
Epoch 11 | Batch 30/100 | Loss 0.211097
Epoch 11 | Batch 40/100 | Loss 0.212853
Epoch 11 | Batch 50/100 | Loss 0.226856
Epoch 11 | Batch 60/100 | Loss 0.227072
Epoch 11 | Batch 70/100 | Loss 0.223481
Epoch 11 | Batch 80/100 | Loss 0.223706
Epoch 11 | Batch 90/100 | Loss 0.230127
100 Test Acc = 68.59% +- 2.24%
Epoch 11: 68.59
Epoch 12 | Batch 0/100 | Loss 0.289767
Epoch 12 | Batch 10/100 | Loss 0.340221
Epoch 12 | Batch 20/100 | Loss 0.348578
Epoch 12 | Batch 30/100 | Loss 0.281964
Epoch 12 | Batch 40/100 | Loss 0.270047
Epoch 12 | Batch 50/100 | Loss 0.258703
Epoch 12 | Batch 60/100 | Loss 0.249219
Epoch 12 | Batch 70/100 | Loss 0.259808
Epoch 12 | Batch 80/100 | Loss 0.258181
Epoch 12 | Batch 90/100 | Loss 0.253004
100 Test Acc = 70.76% +- 1.94%
Epoch 12: 70.76
Epoch 13 | Batch 0/100 | Loss 0.136177
Epoch 13 | Batch 10/100 | Loss 0.251812
Epoch 13 | Batch 20/100 | Loss 0.247327
Epoch 13 | Batch 30/100 | Loss 0.266777
Epoch 13 | Batch 40/100 | Loss 0.271708
Epoch 13 | Batch 50/100 | Loss 0.245965
Epoch 13 | Batch 60/100 | Loss 0.243409
Epoch 13 | Batch 70/100 | Loss 0.227900
Epoch 13 | Batch 80/100 | Loss 0.226962
Epoch 13 | Batch 90/100 | Loss 0.228371
100 Test Acc = 71.21% +- 2.00%
Epoch 13: 71.21
best model! save...
Epoch 14 | Batch 0/100 | Loss 0.028324
Epoch 14 | Batch 10/100 | Loss 0.114177
Epoch 14 | Batch 20/100 | Loss 0.160305
Epoch 14 | Batch 30/100 | Loss 0.196749
Epoch 14 | Batch 40/100 | Loss 0.181343
Epoch 14 | Batch 50/100 | Loss 0.191175
Epoch 14 | Batch 60/100 | Loss 0.212282
Epoch 14 | Batch 70/100 | Loss 0.213792
Epoch 14 | Batch 80/100 | Loss 0.214079
Epoch 14 | Batch 90/100 | Loss 0.210751
100 Test Acc = 71.23% +- 1.86%
Epoch 14: 71.23
best model! save...
Epoch 15 | Batch 0/100 | Loss 0.131766
Epoch 15 | Batch 10/100 | Loss 0.300370
Epoch 15 | Batch 20/100 | Loss 0.292580
Epoch 15 | Batch 30/100 | Loss 0.267851
Epoch 15 | Batch 40/100 | Loss 0.237880
Epoch 15 | Batch 50/100 | Loss 0.226543
Epoch 15 | Batch 60/100 | Loss 0.218906
Epoch 15 | Batch 70/100 | Loss 0.206865
Epoch 15 | Batch 80/100 | Loss 0.203981
Epoch 15 | Batch 90/100 | Loss 0.206633
100 Test Acc = 68.93% +- 2.21%
Epoch 15: 68.93
Epoch 16 | Batch 0/100 | Loss 0.029818
Epoch 16 | Batch 10/100 | Loss 0.182159
Epoch 16 | Batch 20/100 | Loss 0.131978
Epoch 16 | Batch 30/100 | Loss 0.137178
Epoch 16 | Batch 40/100 | Loss 0.163375
Epoch 16 | Batch 50/100 | Loss 0.157279
Epoch 16 | Batch 60/100 | Loss 0.168273
Epoch 16 | Batch 70/100 | Loss 0.160531
Epoch 16 | Batch 80/100 | Loss 0.161691
Epoch 16 | Batch 90/100 | Loss 0.173135
100 Test Acc = 71.11% +- 2.01%
Epoch 16: 71.11
Epoch 17 | Batch 0/100 | Loss 0.056336
Epoch 17 | Batch 10/100 | Loss 0.176227
Epoch 17 | Batch 20/100 | Loss 0.224888
Epoch 17 | Batch 30/100 | Loss 0.211593
Epoch 17 | Batch 40/100 | Loss 0.233860
Epoch 17 | Batch 50/100 | Loss 0.209757
Epoch 17 | Batch 60/100 | Loss 0.198331
Epoch 17 | Batch 70/100 | Loss 0.194301
Epoch 17 | Batch 80/100 | Loss 0.188627
Epoch 17 | Batch 90/100 | Loss 0.199511
100 Test Acc = 67.51% +- 2.35%
Epoch 17: 67.51
Epoch 18 | Batch 0/100 | Loss 0.046477
Epoch 18 | Batch 10/100 | Loss 0.158905
Epoch 18 | Batch 20/100 | Loss 0.179451
Epoch 18 | Batch 30/100 | Loss 0.162504
Epoch 18 | Batch 40/100 | Loss 0.166936
Epoch 18 | Batch 50/100 | Loss 0.174074
Epoch 18 | Batch 60/100 | Loss 0.189076
Epoch 18 | Batch 70/100 | Loss 0.191534
Epoch 18 | Batch 80/100 | Loss 0.193422
Epoch 18 | Batch 90/100 | Loss 0.185617
100 Test Acc = 68.20% +- 2.12%
Epoch 18: 68.20
Epoch 19 | Batch 0/100 | Loss 0.042812
Epoch 19 | Batch 10/100 | Loss 0.097378
Epoch 19 | Batch 20/100 | Loss 0.100758
Epoch 19 | Batch 30/100 | Loss 0.120144
Epoch 19 | Batch 40/100 | Loss 0.161717
Epoch 19 | Batch 50/100 | Loss 0.155232
Epoch 19 | Batch 60/100 | Loss 0.183596
Epoch 19 | Batch 70/100 | Loss 0.169964
Epoch 19 | Batch 80/100 | Loss 0.163655
Epoch 19 | Batch 90/100 | Loss 0.164476
100 Test Acc = 69.52% +- 1.97%
Epoch 19: 69.52
Epoch 20 | Batch 0/100 | Loss 0.368547
Epoch 20 | Batch 10/100 | Loss 0.162147
Epoch 20 | Batch 20/100 | Loss 0.177867
Epoch 20 | Batch 30/100 | Loss 0.220656
Epoch 20 | Batch 40/100 | Loss 0.207395
Epoch 20 | Batch 50/100 | Loss 0.205644
Epoch 20 | Batch 60/100 | Loss 0.201905
Epoch 20 | Batch 70/100 | Loss 0.201652
Epoch 20 | Batch 80/100 | Loss 0.202791
Epoch 20 | Batch 90/100 | Loss 0.199304
100 Test Acc = 70.85% +- 1.94%
Epoch 20: 70.85
Epoch 21 | Batch 0/100 | Loss 0.102771
Epoch 21 | Batch 10/100 | Loss 0.283929
Epoch 21 | Batch 20/100 | Loss 0.219483
Epoch 21 | Batch 30/100 | Loss 0.195123
Epoch 21 | Batch 40/100 | Loss 0.172510
Epoch 21 | Batch 50/100 | Loss 0.168576
Epoch 21 | Batch 60/100 | Loss 0.170856
Epoch 21 | Batch 70/100 | Loss 0.167489
Epoch 21 | Batch 80/100 | Loss 0.163521
Epoch 21 | Batch 90/100 | Loss 0.168675
100 Test Acc = 67.88% +- 1.90%
Epoch 21: 67.88
Epoch 22 | Batch 0/100 | Loss 0.004345
Epoch 22 | Batch 10/100 | Loss 0.096460
Epoch 22 | Batch 20/100 | Loss 0.120542
Epoch 22 | Batch 30/100 | Loss 0.132230
Epoch 22 | Batch 40/100 | Loss 0.147334
Epoch 22 | Batch 50/100 | Loss 0.143507
Epoch 22 | Batch 60/100 | Loss 0.146671
Epoch 22 | Batch 70/100 | Loss 0.138373
Epoch 22 | Batch 80/100 | Loss 0.134263
Epoch 22 | Batch 90/100 | Loss 0.135304
100 Test Acc = 69.01% +- 2.08%
Epoch 22: 69.01
Epoch 23 | Batch 0/100 | Loss 0.135014
Epoch 23 | Batch 10/100 | Loss 0.161723
Epoch 23 | Batch 20/100 | Loss 0.183664
Epoch 23 | Batch 30/100 | Loss 0.174158
Epoch 23 | Batch 40/100 | Loss 0.155803
Epoch 23 | Batch 50/100 | Loss 0.141119
Epoch 23 | Batch 60/100 | Loss 0.152950
Epoch 23 | Batch 70/100 | Loss 0.151938
Epoch 23 | Batch 80/100 | Loss 0.146771
Epoch 23 | Batch 90/100 | Loss 0.147024
100 Test Acc = 70.24% +- 2.01%
Epoch 23: 70.24
Epoch 24 | Batch 0/100 | Loss 0.026144
Epoch 24 | Batch 10/100 | Loss 0.135439
Epoch 24 | Batch 20/100 | Loss 0.194461
Epoch 24 | Batch 30/100 | Loss 0.155411
Epoch 24 | Batch 40/100 | Loss 0.176827
Epoch 24 | Batch 50/100 | Loss 0.184382
Epoch 24 | Batch 60/100 | Loss 0.181631
Epoch 24 | Batch 70/100 | Loss 0.173585
Epoch 24 | Batch 80/100 | Loss 0.166225
Epoch 24 | Batch 90/100 | Loss 0.164918
100 Test Acc = 70.69% +- 1.74%
Epoch 24: 70.69
Epoch 25 | Batch 0/100 | Loss 0.433246
Epoch 25 | Batch 10/100 | Loss 0.138805
Epoch 25 | Batch 20/100 | Loss 0.111026
Epoch 25 | Batch 30/100 | Loss 0.112428
Epoch 25 | Batch 40/100 | Loss 0.106404
Epoch 25 | Batch 50/100 | Loss 0.104740
Epoch 25 | Batch 60/100 | Loss 0.111793
Epoch 25 | Batch 70/100 | Loss 0.107134
Epoch 25 | Batch 80/100 | Loss 0.102210
Epoch 25 | Batch 90/100 | Loss 0.098301
100 Test Acc = 67.11% +- 2.25%
Epoch 25: 67.11
Epoch 26 | Batch 0/100 | Loss 0.060814
Epoch 26 | Batch 10/100 | Loss 0.123085
Epoch 26 | Batch 20/100 | Loss 0.101293
Epoch 26 | Batch 30/100 | Loss 0.102101
Epoch 26 | Batch 40/100 | Loss 0.117918
Epoch 26 | Batch 50/100 | Loss 0.116874
Epoch 26 | Batch 60/100 | Loss 0.120208
Epoch 26 | Batch 70/100 | Loss 0.129701
Epoch 26 | Batch 80/100 | Loss 0.132587
Epoch 26 | Batch 90/100 | Loss 0.136510
100 Test Acc = 68.44% +- 2.11%
Epoch 26: 68.44
Epoch 27 | Batch 0/100 | Loss 0.005995
Epoch 27 | Batch 10/100 | Loss 0.099123
Epoch 27 | Batch 20/100 | Loss 0.118528
Epoch 27 | Batch 30/100 | Loss 0.144544
Epoch 27 | Batch 40/100 | Loss 0.150428
Epoch 27 | Batch 50/100 | Loss 0.158870
Epoch 27 | Batch 60/100 | Loss 0.152206
Epoch 27 | Batch 70/100 | Loss 0.143570
Epoch 27 | Batch 80/100 | Loss 0.143541
Epoch 27 | Batch 90/100 | Loss 0.139421
100 Test Acc = 67.75% +- 2.38%
Epoch 27: 67.75
Epoch 28 | Batch 0/100 | Loss 0.037443
Epoch 28 | Batch 10/100 | Loss 0.197871
Epoch 28 | Batch 20/100 | Loss 0.165928
Epoch 28 | Batch 30/100 | Loss 0.173635
Epoch 28 | Batch 40/100 | Loss 0.168717
Epoch 28 | Batch 50/100 | Loss 0.160220
Epoch 28 | Batch 60/100 | Loss 0.159875
Epoch 28 | Batch 70/100 | Loss 0.164076
Epoch 28 | Batch 80/100 | Loss 0.183110
Epoch 28 | Batch 90/100 | Loss 0.181292
100 Test Acc = 69.43% +- 2.10%
Epoch 28: 69.43
Epoch 29 | Batch 0/100 | Loss 0.299900
Epoch 29 | Batch 10/100 | Loss 0.173833
Epoch 29 | Batch 20/100 | Loss 0.114762
Epoch 29 | Batch 30/100 | Loss 0.147197
Epoch 29 | Batch 40/100 | Loss 0.146580
Epoch 29 | Batch 50/100 | Loss 0.131968
Epoch 29 | Batch 60/100 | Loss 0.134199
Epoch 29 | Batch 70/100 | Loss 0.133743
Epoch 29 | Batch 80/100 | Loss 0.129536
Epoch 29 | Batch 90/100 | Loss 0.134136
100 Test Acc = 68.04% +- 1.96%
Epoch 29: 68.04
Epoch 30 | Batch 0/100 | Loss 0.001132
Epoch 30 | Batch 10/100 | Loss 0.065888
Epoch 30 | Batch 20/100 | Loss 0.122101
Epoch 30 | Batch 30/100 | Loss 0.131155
Epoch 30 | Batch 40/100 | Loss 0.121478
Epoch 30 | Batch 50/100 | Loss 0.128266
Epoch 30 | Batch 60/100 | Loss 0.121495
Epoch 30 | Batch 70/100 | Loss 0.126953
Epoch 30 | Batch 80/100 | Loss 0.125833
Epoch 30 | Batch 90/100 | Loss 0.131267
100 Test Acc = 67.08% +- 1.86%
Epoch 30: 67.08
Epoch 31 | Batch 0/100 | Loss 0.502391
Epoch 31 | Batch 10/100 | Loss 0.148032
Epoch 31 | Batch 20/100 | Loss 0.122804
Epoch 31 | Batch 30/100 | Loss 0.113790
Epoch 31 | Batch 40/100 | Loss 0.123339
Epoch 31 | Batch 50/100 | Loss 0.139296
Epoch 31 | Batch 60/100 | Loss 0.128835
Epoch 31 | Batch 70/100 | Loss 0.137135
Epoch 31 | Batch 80/100 | Loss 0.132294
Epoch 31 | Batch 90/100 | Loss 0.131424
100 Test Acc = 67.68% +- 1.99%
Epoch 31: 67.68
Epoch 32 | Batch 0/100 | Loss 0.126879
Epoch 32 | Batch 10/100 | Loss 0.151442
Epoch 32 | Batch 20/100 | Loss 0.146444
Epoch 32 | Batch 30/100 | Loss 0.147725
Epoch 32 | Batch 40/100 | Loss 0.161210
Epoch 32 | Batch 50/100 | Loss 0.170027
Epoch 32 | Batch 60/100 | Loss 0.166070
Epoch 32 | Batch 70/100 | Loss 0.159540
Epoch 32 | Batch 80/100 | Loss 0.167714
Epoch 32 | Batch 90/100 | Loss 0.162886
100 Test Acc = 67.67% +- 2.20%
Epoch 32: 67.67
Epoch 33 | Batch 0/100 | Loss 0.294981
Epoch 33 | Batch 10/100 | Loss 0.140318
Epoch 33 | Batch 20/100 | Loss 0.187820
Epoch 33 | Batch 30/100 | Loss 0.174133
Epoch 33 | Batch 40/100 | Loss 0.152000
Epoch 33 | Batch 50/100 | Loss 0.155594
Epoch 33 | Batch 60/100 | Loss 0.152381
Epoch 33 | Batch 70/100 | Loss 0.140299
Epoch 33 | Batch 80/100 | Loss 0.141658
Epoch 33 | Batch 90/100 | Loss 0.134435
100 Test Acc = 65.77% +- 2.13%
Epoch 33: 65.77
Epoch 34 | Batch 0/100 | Loss 0.207369
Epoch 34 | Batch 10/100 | Loss 0.099605
Epoch 34 | Batch 20/100 | Loss 0.110794
Epoch 34 | Batch 30/100 | Loss 0.186744
Epoch 34 | Batch 40/100 | Loss 0.163741
Epoch 34 | Batch 50/100 | Loss 0.157867
Epoch 34 | Batch 60/100 | Loss 0.168672
Epoch 34 | Batch 70/100 | Loss 0.168854
Epoch 34 | Batch 80/100 | Loss 0.161834
Epoch 34 | Batch 90/100 | Loss 0.153771
100 Test Acc = 66.15% +- 1.86%
Epoch 34: 66.15
Epoch 35 | Batch 0/100 | Loss 0.132007
Epoch 35 | Batch 10/100 | Loss 0.149822
Epoch 35 | Batch 20/100 | Loss 0.138066
Epoch 35 | Batch 30/100 | Loss 0.132584
Epoch 35 | Batch 40/100 | Loss 0.122580
Epoch 35 | Batch 50/100 | Loss 0.133237
Epoch 35 | Batch 60/100 | Loss 0.118768
Epoch 35 | Batch 70/100 | Loss 0.118872
Epoch 35 | Batch 80/100 | Loss 0.112207
Epoch 35 | Batch 90/100 | Loss 0.110851
100 Test Acc = 67.00% +- 1.96%
Epoch 35: 67.00
Epoch 36 | Batch 0/100 | Loss 0.041389
Epoch 36 | Batch 10/100 | Loss 0.112735
Epoch 36 | Batch 20/100 | Loss 0.097941
Epoch 36 | Batch 30/100 | Loss 0.113385
Epoch 36 | Batch 40/100 | Loss 0.114438
Epoch 36 | Batch 50/100 | Loss 0.122742
Epoch 36 | Batch 60/100 | Loss 0.123151
Epoch 36 | Batch 70/100 | Loss 0.124485
Epoch 36 | Batch 80/100 | Loss 0.117042
Epoch 36 | Batch 90/100 | Loss 0.116386
100 Test Acc = 65.92% +- 1.97%
Epoch 36: 65.92
Epoch 37 | Batch 0/100 | Loss 0.188562
Epoch 37 | Batch 10/100 | Loss 0.143549
Epoch 37 | Batch 20/100 | Loss 0.124153
Epoch 37 | Batch 30/100 | Loss 0.110451
Epoch 37 | Batch 40/100 | Loss 0.105134
Epoch 37 | Batch 50/100 | Loss 0.119805
Epoch 37 | Batch 60/100 | Loss 0.108153
Epoch 37 | Batch 70/100 | Loss 0.117041
Epoch 37 | Batch 80/100 | Loss 0.112016
Epoch 37 | Batch 90/100 | Loss 0.108051
100 Test Acc = 63.32% +- 2.07%
Epoch 37: 63.32
Epoch 38 | Batch 0/100 | Loss 0.118385
Epoch 38 | Batch 10/100 | Loss 0.047736
Epoch 38 | Batch 20/100 | Loss 0.077537
Epoch 38 | Batch 30/100 | Loss 0.089150
Epoch 38 | Batch 40/100 | Loss 0.101659
Epoch 38 | Batch 50/100 | Loss 0.099850
Epoch 38 | Batch 60/100 | Loss 0.106568
Epoch 38 | Batch 70/100 | Loss 0.110895
Epoch 38 | Batch 80/100 | Loss 0.120171
Epoch 38 | Batch 90/100 | Loss 0.124517
100 Test Acc = 69.36% +- 2.09%
Epoch 38: 69.36
Epoch 39 | Batch 0/100 | Loss 0.004348
Epoch 39 | Batch 10/100 | Loss 0.127074
Epoch 39 | Batch 20/100 | Loss 0.098220
Epoch 39 | Batch 30/100 | Loss 0.098842
Epoch 39 | Batch 40/100 | Loss 0.110471
Epoch 39 | Batch 50/100 | Loss 0.109816
Epoch 39 | Batch 60/100 | Loss 0.109390
Epoch 39 | Batch 70/100 | Loss 0.105943
Epoch 39 | Batch 80/100 | Loss 0.111973
Epoch 39 | Batch 90/100 | Loss 0.112452
100 Test Acc = 67.09% +- 2.09%
Epoch 39: 67.09
Epoch 40 | Batch 0/100 | Loss 0.057710
Epoch 40 | Batch 10/100 | Loss 0.104537
Epoch 40 | Batch 20/100 | Loss 0.109000
Epoch 40 | Batch 30/100 | Loss 0.098040
Epoch 40 | Batch 40/100 | Loss 0.105422
Epoch 40 | Batch 50/100 | Loss 0.117511
Epoch 40 | Batch 60/100 | Loss 0.110247
Epoch 40 | Batch 70/100 | Loss 0.107254
Epoch 40 | Batch 80/100 | Loss 0.110099
Epoch 40 | Batch 90/100 | Loss 0.111652
100 Test Acc = 65.13% +- 2.20%
Epoch 40: 65.13
Epoch 41 | Batch 0/100 | Loss 0.101291
Epoch 41 | Batch 10/100 | Loss 0.118695
Epoch 41 | Batch 20/100 | Loss 0.093526
Epoch 41 | Batch 30/100 | Loss 0.113562
Epoch 41 | Batch 40/100 | Loss 0.125252
Epoch 41 | Batch 50/100 | Loss 0.120753
Epoch 41 | Batch 60/100 | Loss 0.115256
Epoch 41 | Batch 70/100 | Loss 0.112340
Epoch 41 | Batch 80/100 | Loss 0.103796
Epoch 41 | Batch 90/100 | Loss 0.103486
100 Test Acc = 66.77% +- 2.05%
Epoch 41: 66.77
Epoch 42 | Batch 0/100 | Loss 0.007593
Epoch 42 | Batch 10/100 | Loss 0.147537
Epoch 42 | Batch 20/100 | Loss 0.153483
Epoch 42 | Batch 30/100 | Loss 0.132198
Epoch 42 | Batch 40/100 | Loss 0.119705
Epoch 42 | Batch 50/100 | Loss 0.116890
Epoch 42 | Batch 60/100 | Loss 0.111188
Epoch 42 | Batch 70/100 | Loss 0.108050
Epoch 42 | Batch 80/100 | Loss 0.108728
Epoch 42 | Batch 90/100 | Loss 0.106299
100 Test Acc = 67.92% +- 2.01%
Epoch 42: 67.92
Epoch 43 | Batch 0/100 | Loss 0.322141
Epoch 43 | Batch 10/100 | Loss 0.099128
Epoch 43 | Batch 20/100 | Loss 0.086201
Epoch 43 | Batch 30/100 | Loss 0.105619
Epoch 43 | Batch 40/100 | Loss 0.103350
Epoch 43 | Batch 50/100 | Loss 0.112856
Epoch 43 | Batch 60/100 | Loss 0.112683
Epoch 43 | Batch 70/100 | Loss 0.115822
Epoch 43 | Batch 80/100 | Loss 0.111868
Epoch 43 | Batch 90/100 | Loss 0.114980
100 Test Acc = 67.20% +- 2.32%
Epoch 43: 67.20
Epoch 44 | Batch 0/100 | Loss 0.033773
Epoch 44 | Batch 10/100 | Loss 0.108831
Epoch 44 | Batch 20/100 | Loss 0.129783
Epoch 44 | Batch 30/100 | Loss 0.156181
Epoch 44 | Batch 40/100 | Loss 0.146754
Epoch 44 | Batch 50/100 | Loss 0.143436
Epoch 44 | Batch 60/100 | Loss 0.137207
Epoch 44 | Batch 70/100 | Loss 0.133561
Epoch 44 | Batch 80/100 | Loss 0.133214
Epoch 44 | Batch 90/100 | Loss 0.140677
100 Test Acc = 65.91% +- 1.94%
Epoch 44: 65.91
Epoch 45 | Batch 0/100 | Loss 0.337704
Epoch 45 | Batch 10/100 | Loss 0.225609
Epoch 45 | Batch 20/100 | Loss 0.168580
Epoch 45 | Batch 30/100 | Loss 0.143734
Epoch 45 | Batch 40/100 | Loss 0.141477
Epoch 45 | Batch 50/100 | Loss 0.129764
Epoch 45 | Batch 60/100 | Loss 0.132121
Epoch 45 | Batch 70/100 | Loss 0.138803
Epoch 45 | Batch 80/100 | Loss 0.142463
Epoch 45 | Batch 90/100 | Loss 0.138090
100 Test Acc = 66.27% +- 2.08%
Epoch 45: 66.27
Epoch 46 | Batch 0/100 | Loss 0.094943
Epoch 46 | Batch 10/100 | Loss 0.058077
Epoch 46 | Batch 20/100 | Loss 0.106529
Epoch 46 | Batch 30/100 | Loss 0.117791
Epoch 46 | Batch 40/100 | Loss 0.107062
Epoch 46 | Batch 50/100 | Loss 0.102214
Epoch 46 | Batch 60/100 | Loss 0.105373
Epoch 46 | Batch 70/100 | Loss 0.112358
Epoch 46 | Batch 80/100 | Loss 0.108878
Epoch 46 | Batch 90/100 | Loss 0.107643
100 Test Acc = 66.35% +- 1.99%
Epoch 46: 66.35
Epoch 47 | Batch 0/100 | Loss 0.221890
Epoch 47 | Batch 10/100 | Loss 0.137990
Epoch 47 | Batch 20/100 | Loss 0.194539
Epoch 47 | Batch 30/100 | Loss 0.161288
Epoch 47 | Batch 40/100 | Loss 0.148357
Epoch 47 | Batch 50/100 | Loss 0.144159
Epoch 47 | Batch 60/100 | Loss 0.136358
Epoch 47 | Batch 70/100 | Loss 0.132167
Epoch 47 | Batch 80/100 | Loss 0.124680
Epoch 47 | Batch 90/100 | Loss 0.120063
100 Test Acc = 66.67% +- 2.03%
Epoch 47: 66.67
Epoch 48 | Batch 0/100 | Loss 0.197950
Epoch 48 | Batch 10/100 | Loss 0.068751
Epoch 48 | Batch 20/100 | Loss 0.077635
Epoch 48 | Batch 30/100 | Loss 0.102853
Epoch 48 | Batch 40/100 | Loss 0.121656
Epoch 48 | Batch 50/100 | Loss 0.107171
Epoch 48 | Batch 60/100 | Loss 0.107443
Epoch 48 | Batch 70/100 | Loss 0.104977
Epoch 48 | Batch 80/100 | Loss 0.095330
Epoch 48 | Batch 90/100 | Loss 0.090621
100 Test Acc = 67.60% +- 2.13%
Epoch 48: 67.60
Epoch 49 | Batch 0/100 | Loss 0.018381
Epoch 49 | Batch 10/100 | Loss 0.103506
Epoch 49 | Batch 20/100 | Loss 0.108916
Epoch 49 | Batch 30/100 | Loss 0.111410
Epoch 49 | Batch 40/100 | Loss 0.109695
Epoch 49 | Batch 50/100 | Loss 0.113195
Epoch 49 | Batch 60/100 | Loss 0.108199
Epoch 49 | Batch 70/100 | Loss 0.101982
Epoch 49 | Batch 80/100 | Loss 0.101287
Epoch 49 | Batch 90/100 | Loss 0.106391
100 Test Acc = 68.35% +- 1.92%
Epoch 49: 68.35
Epoch 50 | Batch 0/100 | Loss 0.016678
Epoch 50 | Batch 10/100 | Loss 0.095608
Epoch 50 | Batch 20/100 | Loss 0.083842
Epoch 50 | Batch 30/100 | Loss 0.089429
Epoch 50 | Batch 40/100 | Loss 0.118014
Epoch 50 | Batch 50/100 | Loss 0.123779
Epoch 50 | Batch 60/100 | Loss 0.119366
Epoch 50 | Batch 70/100 | Loss 0.123063
Epoch 50 | Batch 80/100 | Loss 0.110745
Epoch 50 | Batch 90/100 | Loss 0.108093
100 Test Acc = 68.36% +- 2.09%
Epoch 50: 68.36
Epoch 51 | Batch 0/100 | Loss 0.001785
Epoch 51 | Batch 10/100 | Loss 0.090333
Epoch 51 | Batch 20/100 | Loss 0.098864
Epoch 51 | Batch 30/100 | Loss 0.088720
Epoch 51 | Batch 40/100 | Loss 0.090332
Epoch 51 | Batch 50/100 | Loss 0.090403
Epoch 51 | Batch 60/100 | Loss 0.084270
Epoch 51 | Batch 70/100 | Loss 0.082233
Epoch 51 | Batch 80/100 | Loss 0.090238
Epoch 51 | Batch 90/100 | Loss 0.088671
100 Test Acc = 65.24% +- 2.02%
Epoch 51: 65.24
Epoch 52 | Batch 0/100 | Loss 0.061124
Epoch 52 | Batch 10/100 | Loss 0.056888
Epoch 52 | Batch 20/100 | Loss 0.062773
Epoch 52 | Batch 30/100 | Loss 0.074291
Epoch 52 | Batch 40/100 | Loss 0.075815
Epoch 52 | Batch 50/100 | Loss 0.077112
Epoch 52 | Batch 60/100 | Loss 0.075089
Epoch 52 | Batch 70/100 | Loss 0.080039
Epoch 52 | Batch 80/100 | Loss 0.088371
Epoch 52 | Batch 90/100 | Loss 0.085575
100 Test Acc = 67.47% +- 1.79%
Epoch 52: 67.47
Epoch 53 | Batch 0/100 | Loss 0.157711
Epoch 53 | Batch 10/100 | Loss 0.084123
Epoch 53 | Batch 20/100 | Loss 0.067470
Epoch 53 | Batch 30/100 | Loss 0.083847
Epoch 53 | Batch 40/100 | Loss 0.080657
Epoch 53 | Batch 50/100 | Loss 0.102415
Epoch 53 | Batch 60/100 | Loss 0.090640
Epoch 53 | Batch 70/100 | Loss 0.093966
Epoch 53 | Batch 80/100 | Loss 0.107512
Epoch 53 | Batch 90/100 | Loss 0.104303
100 Test Acc = 67.89% +- 2.03%
Epoch 53: 67.89
Epoch 54 | Batch 0/100 | Loss 0.060889
Epoch 54 | Batch 10/100 | Loss 0.113880
Epoch 54 | Batch 20/100 | Loss 0.112749
Epoch 54 | Batch 30/100 | Loss 0.097905
Epoch 54 | Batch 40/100 | Loss 0.110665
Epoch 54 | Batch 50/100 | Loss 0.110209
Epoch 54 | Batch 60/100 | Loss 0.110571
Epoch 54 | Batch 70/100 | Loss 0.111062
Epoch 54 | Batch 80/100 | Loss 0.114610
Epoch 54 | Batch 90/100 | Loss 0.114494
100 Test Acc = 67.96% +- 2.08%
Epoch 54: 67.96
Epoch 55 | Batch 0/100 | Loss 0.010886
Epoch 55 | Batch 10/100 | Loss 0.070023
Epoch 55 | Batch 20/100 | Loss 0.068390
Epoch 55 | Batch 30/100 | Loss 0.089067
Epoch 55 | Batch 40/100 | Loss 0.078897
Epoch 55 | Batch 50/100 | Loss 0.091953
Epoch 55 | Batch 60/100 | Loss 0.095335
Epoch 55 | Batch 70/100 | Loss 0.091454
Epoch 55 | Batch 80/100 | Loss 0.089972
Epoch 55 | Batch 90/100 | Loss 0.086789
100 Test Acc = 65.93% +- 2.04%
Epoch 55: 65.93
Epoch 56 | Batch 0/100 | Loss 0.086514
Epoch 56 | Batch 10/100 | Loss 0.072031
Epoch 56 | Batch 20/100 | Loss 0.063385
Epoch 56 | Batch 30/100 | Loss 0.070210
Epoch 56 | Batch 40/100 | Loss 0.061792
Epoch 56 | Batch 50/100 | Loss 0.068731
Epoch 56 | Batch 60/100 | Loss 0.074788
Epoch 56 | Batch 70/100 | Loss 0.077736
Epoch 56 | Batch 80/100 | Loss 0.081196
Epoch 56 | Batch 90/100 | Loss 0.083346
100 Test Acc = 67.52% +- 1.82%
Epoch 56: 67.52
Epoch 57 | Batch 0/100 | Loss 0.144664
Epoch 57 | Batch 10/100 | Loss 0.076794
Epoch 57 | Batch 20/100 | Loss 0.093363
Epoch 57 | Batch 30/100 | Loss 0.104080
Epoch 57 | Batch 40/100 | Loss 0.094039
Epoch 57 | Batch 50/100 | Loss 0.093303
Epoch 57 | Batch 60/100 | Loss 0.096706
Epoch 57 | Batch 70/100 | Loss 0.102250
Epoch 57 | Batch 80/100 | Loss 0.104941
Epoch 57 | Batch 90/100 | Loss 0.102727
100 Test Acc = 67.04% +- 2.35%
Epoch 57: 67.04
Epoch 58 | Batch 0/100 | Loss 0.162867
Epoch 58 | Batch 10/100 | Loss 0.106033
Epoch 58 | Batch 20/100 | Loss 0.116211
Epoch 58 | Batch 30/100 | Loss 0.108487
Epoch 58 | Batch 40/100 | Loss 0.091842
Epoch 58 | Batch 50/100 | Loss 0.084800
Epoch 58 | Batch 60/100 | Loss 0.081834
Epoch 58 | Batch 70/100 | Loss 0.080600
Epoch 58 | Batch 80/100 | Loss 0.078451
Epoch 58 | Batch 90/100 | Loss 0.075669
100 Test Acc = 67.20% +- 2.16%
Epoch 58: 67.20
Epoch 59 | Batch 0/100 | Loss 0.044624
Epoch 59 | Batch 10/100 | Loss 0.145978
Epoch 59 | Batch 20/100 | Loss 0.118178
Epoch 59 | Batch 30/100 | Loss 0.092982
Epoch 59 | Batch 40/100 | Loss 0.102951
Epoch 59 | Batch 50/100 | Loss 0.101219
Epoch 59 | Batch 60/100 | Loss 0.111118
Epoch 59 | Batch 70/100 | Loss 0.112158
Epoch 59 | Batch 80/100 | Loss 0.119686
Epoch 59 | Batch 90/100 | Loss 0.115251
100 Test Acc = 65.17% +- 2.13%
Epoch 59: 65.17
Checkpoint directory: checkpoints/method_matchingnet_dataset_swissprot_n_shot_5/swissprot/matchingnet_FCNet

**WARNING: NO VERSION LINE FOUND IN GAF FILE. USING:
!gaf-version: 2.2
[2023-12-09 01:37:13,647][root][ERROR] - Failed to validate header as GAF v2.2:
[]
HMS:0:00:06.562698 310,057 annotations READ: ./data/swissprot/filtered_goa_uniprot_all_noiea.gaf 
25933 IDs in loaded association branch, BP
Using checkpoint dir: checkpoints/method_matchingnet_dataset_swissprot_n_shot_5/swissprot/matchingnet_FCNet/20231209_013223
/home/said.gurbuz/epfl-dl-in-biomed-project/methods/matchingnet.py:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  a = self.softmax(logit_a)
/home/said.gurbuz/epfl-dl-in-biomed-project/methods/matchingnet.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  softmax = self.softmax(scores)
600 Test Acc = 92.59% +- 0.56%

**WARNING: NO VERSION LINE FOUND IN GAF FILE. USING:
!gaf-version: 2.2
[2023-12-09 01:38:50,014][root][ERROR] - Failed to validate header as GAF v2.2:
[]
HMS:0:00:07.708181 310,057 annotations READ: ./data/swissprot/filtered_goa_uniprot_all_noiea.gaf 
25933 IDs in loaded association branch, BP
Using checkpoint dir: checkpoints/method_matchingnet_dataset_swissprot_n_shot_5/swissprot/matchingnet_FCNet/20231209_013223
/home/said.gurbuz/epfl-dl-in-biomed-project/methods/matchingnet.py:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  a = self.softmax(logit_a)
/home/said.gurbuz/epfl-dl-in-biomed-project/methods/matchingnet.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  softmax = self.softmax(scores)
600 Test Acc = 70.52% +- 0.78%

**WARNING: NO VERSION LINE FOUND IN GAF FILE. USING:
!gaf-version: 2.2
[2023-12-09 01:39:34,496][root][ERROR] - Failed to validate header as GAF v2.2:
[]
HMS:0:00:06.977967 310,057 annotations READ: ./data/swissprot/filtered_goa_uniprot_all_noiea.gaf 
25933 IDs in loaded association branch, BP
Using checkpoint dir: checkpoints/method_matchingnet_dataset_swissprot_n_shot_5/swissprot/matchingnet_FCNet/20231209_013223
/home/said.gurbuz/epfl-dl-in-biomed-project/methods/matchingnet.py:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  a = self.softmax(logit_a)
/home/said.gurbuz/epfl-dl-in-biomed-project/methods/matchingnet.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  softmax = self.softmax(scores)
600 Test Acc = 62.20% +- 0.62%
Results logged to ./checkpoints/method_matchingnet_dataset_swissprot_n_shot_5/results.txt
+-------+--------------------+-------------------+
| split |      acc_mean      |      acc_std      |
+-------+--------------------+-------------------+
| train | 92.59333333333333  | 7.004376409709069 |
|  val  |       70.52        | 9.713334095935302 |
|  test | 62.204444444444455 | 7.710674339215222 |
+-------+--------------------+-------------------+
